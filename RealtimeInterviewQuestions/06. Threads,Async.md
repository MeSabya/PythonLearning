## Build a producer-consumer pipeline in Python using asyncio.

<details>

```python
import asyncio
import random

# Create a shared queue
q = asyncio.Queue(maxsize=5)

# Producer coroutine
async def producer():
    for i in range(10):
        await asyncio.sleep(random.uniform(0.1, 0.5))  # Simulate data production
        item = f"item-{i}"
        await q.put(item)  # Automatically waits if queue is full
        print(f"[Producer] Produced {item}")
    await q.put(None)  # Sentinel to signal completion

# Consumer coroutine
async def consumer():
    while True:
        item = await q.get()
        if item is None:  # Exit signal
            break
        print(f"[Consumer] Consuming {item}")
        await asyncio.sleep(random.uniform(0.1, 0.4))  # Simulate work
        q.task_done()

# Main coroutine
async def main():
    producer_task = asyncio.create_task(producer())
    consumer_task = asyncio.create_task(consumer())

    await asyncio.gather(producer_task, consumer_task)
    await q.join()  # Wait until all tasks are marked done

# Run the event loop
asyncio.run(main())
```

</details>


## threading.Thread Vs ThreadPoolExecutor

<details>

### ✅ threading.Thread
- Low-level: You manually create, start, and join threads. 
- No return values: Worker functions don't return values directly.
- Use case: Best when you need custom control over thread behavior or synchronization (e.g., locks, events).
- Manual management: You handle thread lifecycle explicitly.

### ✅ ThreadPoolExecutor
- High-level: Manages a pool of reusable threads.
- Built-in Future support: Easily retrieve results or exceptions using .result().
- Efficient for many short tasks: Automatically queues and runs tasks in parallel.
- Use case: Ideal for I/O-bound task parallelization (e.g., web requests, file I/O).

</details>

## What are Orphan Coroutines?

<details>

An orphan coroutine is:
A coroutine that is created (i.e., async_fn() is called) but never awaited or scheduled to run — so it sits around doing nothing and is never executed.

### Coroutine created in loop but not collected

```python
async def work(i):
    await asyncio.sleep(1)
    print(f"Work {i} done")

async def main():
    for i in range(5):
        work(i)  # ❌ No await or scheduling

asyncio.run(main())
```
### Fix:

```python
await asyncio.gather(*(work(i) for i in range(5)))
```
### Coroutine launched in a function that’s not async

```python
def fire_and_forget():
    fetch_data()  # ❌ Not awaited and not scheduled

async def fetch_data():
    await asyncio.sleep(1)
    print("Done")

fire_and_forget()
```
</details>

## Can a normal function call async function and vice versa ?
<details>

Directly? No.
If you call an async def function from a regular function, you just get a coroutine object, not a result.

- ✅ You can call it — but you must schedule it or run it using an event loop.
- ✅ Example: What doesn't work

```python
async def say_hello():
    print("Hello")

def main():
    say_hello()  # ❌ This returns a coroutine object — it does NOT run

main()
```

Output: Nothing
Why? say_hello() returns a coroutine object; it needs to be awaited or scheduled.

### Realworld Problem-1
Suppose you're writing a command-line tool (CLI) using argparse, and your tool needs to make async HTTP calls (e.g., using aiohttp).
Since CLI tools are synchronous, but aiohttp is async, you need to bridge between them.

```python
import asyncio
import aiohttp
import argparse

# --- Async function ---
async def fetch_url(url):
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            return await response.text()

# --- Sync wrapper ---
def fetch_url_sync(url):
    return asyncio.run(fetch_url(url))  # Bridging point

# --- CLI using argparse (sync code) ---
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("url", help="URL to fetch")
    args = parser.parse_args()

    html = fetch_url_sync(args.url)
    print(html[:300])  # Print first 300 chars

if __name__ == "__main__":
    main()
```

</details>
